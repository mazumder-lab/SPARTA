{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72120713-676b-4d71-addd-a33b276d85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from math import floor\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.cuda\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataset_utils import get_train_and_test_dataloader\n",
    "from models.resnet import ResNet50, ResNet18\n",
    "from models.wide_resnet import Wide_ResNet\n",
    "from optimizers.optimizer_utils import use_optimizer, use_lr_scheduler\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7c3423-fb33-424f-a491-60365678b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=100)\n",
    "net.train()\n",
    "net = ModuleValidator.fix(net.to(\"cpu\"))  # Note that we are using the backbone as a black-box featurizer. It's never trained, so we can keep BatchNorms in there.\n",
    "net.load_state_dict(torch.load(\"./model18_100_GN-Copy1.pt\", map_location=\"cpu\"))\n",
    "net = net.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ba807e-b63d-43a4-82f5-bd7af1fde614",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_names = [name for name, p in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a109e69-6bac-410e-8aed-1525a20c18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "classifier_names = ls_names[-2:]\n",
    "conv_norm_names = ls_names[-8:-2]\n",
    "classifier_params, conv_norm_params = [], []\n",
    "for name, param in net.named_parameters():\n",
    "    if name in classifier_names:\n",
    "        classifier_params.append(param)\n",
    "    elif name in conv_norm_names:\n",
    "        conv_norm_params.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b35ee6a-1638-4cbc-8005-131ab65d081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, all_param_flag=False):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad or all_param_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79493db-06fd-4f47-a409-741e0eb0bfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4771940"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8373f0d-77cd-4b96-9605-f66547509965",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "net_params = [{\"params\" : classifier_params, \"lr\" : 0.8}, {\"params\" : conv_norm_params, \"lr\" : 0.01}]\n",
    "optimizer = torch.optim.SGD(net_params, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f55c03-c632-42f4-bec3-f55d2c4e723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIF100_MEAN = [0.5071, 0.4867, 0.4408]\n",
    "CIF100_STD = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "batch_size = 1000\n",
    "train_ds = CIFAR10('../datasets/', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=Compose([ToTensor(), Normalize(CIF100_MEAN, CIF100_STD)])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4, \n",
    "                          pin_memory=True,\n",
    ")\n",
    "\n",
    "test_ds = CIFAR10('../datasets/', \n",
    "                  train=False, \n",
    "                  download=True, \n",
    "                  transform=Compose([ToTensor(), Normalize(CIF100_MEAN, CIF100_STD)])\n",
    ")\n",
    "test_loader = DataLoader(test_ds, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False, \n",
    "                         num_workers=2, \n",
    "                         pin_memory=True\n",
    ")\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a38e1389-b92f-4fdc-aa56-1de7791a8b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmakni/.conda/envs/pruning/lib/python3.9/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/gridsan/mmakni/.conda/envs/pruning/lib/python3.9/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ConstantLR, LambdaLR\n",
    "epochs = 200\n",
    "\n",
    "privacy_engine = opacus.PrivacyEngine()\n",
    "(\n",
    "model,\n",
    "optimizer,\n",
    "train_loader,\n",
    ") = privacy_engine.make_private_with_epsilon(\n",
    "module=net,\n",
    "optimizer=optimizer,\n",
    "data_loader=train_loader,\n",
    "epochs=epochs,\n",
    "target_epsilon=1,\n",
    "target_delta=1e-5,\n",
    "max_grad_norm=1,\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec8fc196-5fe9-4b1c-bd62-949cba7c4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader):\n",
    "  accs = []\n",
    "  losses = []\n",
    "  for x, y in tqdm(train_loader):\n",
    "    x = x.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    preds = logits.argmax(-1)\n",
    "    n_correct = float(preds.eq(y).sum())\n",
    "    batch_accuracy = n_correct / len(y)\n",
    "\n",
    "    accs.append(batch_accuracy)\n",
    "    losses.append(float(loss))\n",
    "\n",
    "  print(\n",
    "      f\"Train Accuracy: {mean(accs):.6f}\"\n",
    "      f\"Train Loss: {mean(losses):.6f}\"\n",
    "  ) \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ae80aa-0d4a-4248-a646-e25610fc6970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79941d2b-6571-4637-a4f9-954fe5eb8c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340677e1d4d84e58b21dccb5fcfdf64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eee49c3e20e4c9ab9aa36d8eb9674f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmakni/.conda/envs/pruning/lib/python3.9/site-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.75 GiB (GPU 0; 31.74 GiB total capacity; 24.52 GiB already allocated; 4.95 GiB free; 25.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(scheduler\u001b[38;5;241m.\u001b[39mget_last_lr())\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()    \n",
      "File \u001b[0;32m~/.conda/envs/pruning/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pruning/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pruning/lib/python3.9/site-packages/torch/nn/modules/module.py:62\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pruning/lib/python3.9/site-packages/opacus/grad_sample/grad_sample_module.py:337\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     grad_sampler_fn \u001b[38;5;241m=\u001b[39m ft_compute_per_sample_gradient\n\u001b[0;32m--> 337\u001b[0m grad_samples \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_sampler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackprops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, gs \u001b[38;5;129;01min\u001b[39;00m grad_samples\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    339\u001b[0m     create_or_accumulate_grad_sample(\n\u001b[1;32m    340\u001b[0m         param\u001b[38;5;241m=\u001b[39mparam, grad_sample\u001b[38;5;241m=\u001b[39mgs, max_batch_len\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mmax_batch_len\n\u001b[1;32m    341\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pruning/lib/python3.9/site-packages/opacus/grad_sample/conv.py:103\u001b[0m, in \u001b[0;36mcompute_conv_grad_sample\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# rearrange the above tensor and extract diagonals.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m grad_sample \u001b[38;5;241m=\u001b[39m grad_sample\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     96\u001b[0m     n,\n\u001b[1;32m     97\u001b[0m     layer\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     np\u001b[38;5;241m.\u001b[39mprod(layer\u001b[38;5;241m.\u001b[39mkernel_size),\n\u001b[1;32m    102\u001b[0m )\n\u001b[0;32m--> 103\u001b[0m grad_sample \u001b[38;5;241m=\u001b[39m \u001b[43mcontract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mngrg...->ngr...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m shape \u001b[38;5;241m=\u001b[39m [n] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    105\u001b[0m ret[layer\u001b[38;5;241m.\u001b[39mweight] \u001b[38;5;241m=\u001b[39m grad_sample\u001b[38;5;241m.\u001b[39mview(shape)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.75 GiB (GPU 0; 31.74 GiB total capacity; 24.52 GiB already allocated; 4.95 GiB free; 25.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    train(net, criterion, optimizer, train_loader)\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pruning)",
   "language": "python",
   "name": "pruning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
