{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72120713-676b-4d71-addd-a33b276d85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from math import floor\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.cuda\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataset_utils import get_train_and_test_dataloader\n",
    "from models.resnet import ResNet50, ResNet18\n",
    "from models.wide_resnet import Wide_ResNet\n",
    "from optimizers.optimizer_utils import use_optimizer, use_lr_scheduler\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c3423-fb33-424f-a491-60365678b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=100)\n",
    "net.train()\n",
    "net = ModuleValidator.fix(net.to(\"cpu\"))  # Note that we are using the backbone as a black-box featurizer. It's never trained, so we can keep BatchNorms in there.\n",
    "net.load_state_dict(torch.load(\"./model18_100_GN-Copy1.pt\", map_location=\"cpu\"))\n",
    "net = net.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba807e-b63d-43a4-82f5-bd7af1fde614",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_names = [name for name, p in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a109e69-6bac-410e-8aed-1525a20c18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "classifier_names = ls_names[-2:]\n",
    "conv_norm_names = ls_names[-8:-2]\n",
    "classifier_params, conv_norm_params = [], []\n",
    "for name, param in net.named_parameters():\n",
    "    if name in classifier_names:\n",
    "        classifier_params.append(param)\n",
    "    elif name in conv_norm_names:\n",
    "        conv_norm_params.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35ee6a-1638-4cbc-8005-131ab65d081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, all_param_flag=False):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad or all_param_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79493db-06fd-4f47-a409-741e0eb0bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8373f0d-77cd-4b96-9605-f66547509965",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "net_params = [{\"params\" : classifier_params, \"lr\" : 0.8}, {\"params\" : conv_norm_params, \"lr\" : 0.01}]\n",
    "optimizer = torch.optim.SGD(net_params, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f55c03-c632-42f4-bec3-f55d2c4e723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIF100_MEAN = [0.5071, 0.4867, 0.4408]\n",
    "CIF100_STD = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "batch_size = 500\n",
    "train_ds = CIFAR10('../datasets/', \n",
    "                   train=True, \n",
    "                   download=True, \n",
    "                   transform=Compose([ToTensor(), Normalize(CIF100_MEAN, CIF100_STD)])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4, \n",
    "                          pin_memory=True,\n",
    ")\n",
    "\n",
    "test_ds = CIFAR10('../datasets/', \n",
    "                  train=False, \n",
    "                  download=True, \n",
    "                  transform=Compose([ToTensor(), Normalize(CIF100_MEAN, CIF100_STD)])\n",
    ")\n",
    "test_loader = DataLoader(test_ds, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False, \n",
    "                         num_workers=2, \n",
    "                         pin_memory=True\n",
    ")\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e1389-b92f-4fdc-aa56-1de7791a8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ConstantLR, LambdaLR\n",
    "epochs = 200\n",
    "\n",
    "privacy_engine = opacus.PrivacyEngine()\n",
    "(\n",
    "model,\n",
    "optimizer,\n",
    "train_loader,\n",
    ") = privacy_engine.make_private_with_epsilon(\n",
    "module=net,\n",
    "optimizer=optimizer,\n",
    "data_loader=train_loader,\n",
    "epochs=epochs,\n",
    "target_epsilon=1,\n",
    "target_delta=1e-5,\n",
    "max_grad_norm=1,\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fc196-5fe9-4b1c-bd62-949cba7c4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader):\n",
    "  accs = []\n",
    "  losses = []\n",
    "  for x, y in tqdm(train_loader):\n",
    "    x = x.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    preds = logits.argmax(-1)\n",
    "    n_correct = float(preds.eq(y).sum())\n",
    "    batch_accuracy = n_correct / len(y)\n",
    "\n",
    "    accs.append(batch_accuracy)\n",
    "    losses.append(float(loss))\n",
    "\n",
    "  print(\n",
    "      f\"Train Accuracy: {mean(accs):.6f}\"\n",
    "      f\"Train Loss: {mean(losses):.6f}\"\n",
    "  ) \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae80aa-0d4a-4248-a646-e25610fc6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79941d2b-6571-4637-a4f9-954fe5eb8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    train(net, criterion, optimizer, train_loader)\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b50790-523e-4f1e-a8e1-260de6b76719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def compute_acc(model, dataloader, device=\"cpu\"):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"The device used is {device}.\", flush=True)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            if len(labels.shape) >= 2:\n",
    "                labels = labels[:, 0]\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            del images, labels, outputs\n",
    "\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcdd9a-e551-40ec-98e4-92de3ff99e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc(model, test_loader, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pruning)",
   "language": "python",
   "name": "pruning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
